# -*- coding: utf-8 -*-
"""Narsi_Reddy_Konapalli_1_9_2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QyRSK4wtOXoQ6742ZruKjCd6E26sJPfM
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Section 1 - Funnel Analysis

**Identify and appropriately handle the missing/blank and duplicate values in the dataset, and explain the logic behind your strategy in a short paragraph.**
"""

import pandas as pd
funnel = pd.read_excel('/content/drive/MyDrive/FittLyf Assessment/AssignmentData.xlsx')
funnel.head()

print(funnel.describe())
print(funnel.info())

funnel['Date'] = pd.to_datetime(funnel['Date'])
columns_to_convert = ['Targeted Productivity', 'Overtime', 'No. of Workers', 'Actual Productivity']
for column in columns_to_convert:
    # Converting to numeric, setting non-numeric values to NaN
    funnel[column] = pd.to_numeric(funnel[column], errors='coerce')
    # Calculating the mean of the column, ignoring NaN values
    mean_value = funnel[column].mean()
    # Replacing NaN values with the mean of the column
    funnel[column].fillna(mean_value, inplace=True)

# Checking for duplicates
print('Duplicates: ',funnel.duplicated().sum())
# Checking for missing values
print('Null:\n',funnel.isnull().sum())
# Dropping duplicates if any
funnel.drop_duplicates(inplace=True)
# Filling missing values in 'Actual Productivity' with mean value
funnel['Actual Productivity'].fillna(funnel['Actual Productivity'].mean(), inplace=True)

funnel.info()

"""**Principal Component Analysis (PCA)**"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
# Selecting features for PCA
features = ['Targeted Productivity', 'Overtime', 'No. of Workers', 'Actual Productivity']
X = funnel[features]
# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Applying PCA
pca = PCA()
pca.fit(X_scaled)
# Calculating explained variance
explained_variance = pca.explained_variance_ratio_
cumulative_variance = explained_variance.cumsum()

"""The principal components represent new, uncorrelated features that capture the most important
variations in the original data. The first principal component explains the most variance,
followed by the second, and so on.
"""

import numpy as np
n_components = np.argmax(cumulative_variance >=0.90 ) + 1
print("Number of components explaining 90% variance:", n_components)

cumulative_variance

# Plotting explained variance
plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by Principal Components')
plt.show()

"""The PCA results indicate that the first three principal components explain about 93.35% of the variance
in the data, with the first component alone accounting for 45.01%. This suggests that we can focus on these
components to understand the key factors influencing productivity in the organization. By reducing the dataset
to these components, we simplify the analysis while still capturing most of the important information,
making it easier to identify and address the primary drivers of productivity.
"""

import matplotlib.pyplot as plt
plt.figure(figsize=(15,6))
funnel['Actual Productivity'].plot()

"""**Predictive Modeling and Time Series Analysis**"""

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
from math import sqrt
funnel = funnel.set_index('Date')
ts = funnel['Actual Productivity']
# Splitting data for training and testing
train_data = ts[:-4]
test_data = ts[-4:]
# Fitting ARIMA model
model = ARIMA(train_data, order=(1, 1, 1))
model_fit = model.fit()
# Forecast
forecast = model_fit.forecast(steps=4)

from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
# Calculatting MAPE
mape = mean_absolute_percentage_error(test_data, forecast)
print('MAPE:', mape)
# Calculating MSE
mse = mean_squared_error(test_data, forecast)
print('MSE:', mse)

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.plot( test_data.values, label='Actual')
plt.plot( forecast.values, label='Forecast')
plt.xlabel('Date')
plt.ylabel('Actual Productivity')
plt.title('Actual vs Forecasted Productivity')
plt.legend()
plt.show()

"""Evaluated the ARIMA model's forecasts by calculating the
Mean Absolute Percentage Error (MAPE), which was 33.8%, and the Mean Squared Error (MSE), which was 0.035.

**Clustering Analysis**
"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
# Selecting features for clustering
features_for_clustering = ['Actual Productivity', 'Overtime', 'No. of Workers']
X_clustering = funnel[features_for_clustering]
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X_clustering)
    inertia.append(kmeans.inertia_)

# Plotting the Elbow method graph
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

#Based on the Elbow method, I determined that the optimal number of clusters is 3
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
funnel['Cluster'] = kmeans.fit_predict(X_clustering)

import matplotlib.pyplot as plt
# Summing productivity by cluster
cluster_productivity = funnel.groupby('Cluster')['Actual Productivity'].sum()
# Creating pie chart
plt.pie(cluster_productivity, labels=cluster_productivity.index, autopct='%1.1f%%', startangle=90)
plt.title('Cluster Contribution to Total Productivity')
plt.show()

print('cluster_productivity: ',cluster_productivity)

"""The productivity data reveals that Cluster 0, with a value of 425.83, is the most productive group, likely consisting of the most efficient workers. Cluster 1, with a productivity of 314.11, represents a moderately productive segment, contributing positively but not as strongly. Lastly, Cluster 2, with a productivity of 111.87

**Budget Allocation Strategy (Bonus)**
"""

department_quarterly_spend = 8.4
# Calculating Department Value for each row
funnel['Department Value'] = funnel['Actual Productivity'] / department_quarterly_spend
# Analyzing the value brought by each department
department_value_analysis = funnel.groupby('Cluster')['Department Value'].agg(['mean', 'sum'])
print(department_value_analysis)

# Calculating the productivity per lakh spent for each cluster
productivity_per_lakh = funnel.groupby('Cluster')['Actual Productivity'].sum() / department_quarterly_spend
# Normalizing the productivity per lakh to get a proportion for budget allocation
total_productivity_per_lakh = productivity_per_lakh.sum()
budget_allocation_proportions = productivity_per_lakh / total_productivity_per_lakh
# Calculating the revised budget allocation for each cluster
revised_budget_allocation = budget_allocation_proportions * department_quarterly_spend
print("Revised Budget Allocation:")
for cluster_id, allocation in revised_budget_allocation.items():
    print(f"Cluster {cluster_id}: {allocation:.2f} lakhs")

"""The revised budget allocation strategy is based on the principle of allocating resources
proportionally to the productivity generated per unit of expenditure.
By analyzing the productivity per lakh spent for each cluster, we identified the clusters
that generate the most output for each unit of investment. The revised budget allocates a
larger share of resources to these high-performing clusters, aiming to maximize the overall
productivity.

# Section 2 - Anomaly detection

**Data Import and Exploration**
"""

import pandas as pd
transactions = pd.read_excel('/content/drive/MyDrive/FittLyf Assessment/AssignmentData.xlsx',sheet_name='creditcard')
transactions.head()

transactions.info()

for col in transactions.columns:
    # Checking if the column is of object type
    if transactions[col].dtype == 'object':
        # Converting to numeric, coercing errors to NaN
        transactions[col] = pd.to_numeric(transactions[col], errors='coerce')
        # Filling NaN values with the mean of the column
        mean_value = transactions[col].mean()
        transactions[col].fillna(mean_value, inplace=True)

transactions['Class'].value_counts()

# Calculating the percentage of fraudulent transactions
class_distribution = transactions['Class'].value_counts()
fraud_percentage = (class_distribution[1] / class_distribution.sum()) * 100
print(f"Percentage of fraudulent transactions: {fraud_percentage:.2f}%")

# Visualizing the class distribution
import matplotlib.pyplot as plt
plt.bar(class_distribution.index, class_distribution.values)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Distribution of Fraudulent vs. Non-Fraudulent Transactions')
plt.xticks([0, 1], ['Non-Fraudulent', 'Fraudulent'])
plt.show()

# Summary statistics for numerical features
print(transactions.describe())

import seaborn as sns
plt.figure(figsize=(15,40))
for i in range(1,29):
  plt.subplot(7,4,i)
  col=transactions.columns[i]
  sns.histplot(transactions[col],kde=True)
  plt.title(col)
plt.show()

import matplotlib.pyplot as plt
# Creating separate DataFrames for fraudulent and non-fraudulent transactions
fraudulent = transactions[transactions['Class'] == 1]
non_fraudulent = transactions[transactions['Class'] == 0]

# Plotting histograms of transaction amounts
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plt.hist(fraudulent['Amount'], bins=50)
plt.title('Fraudulent Transactions')
plt.xlabel('Amount')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.hist(non_fraudulent['Amount'], bins=50)
plt.title('Non-Fraudulent Transactions')
plt.xlabel('Amount')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""**Feature Engineering**"""

from sklearn.preprocessing import StandardScaler
# Initializing the scaler
scaler = StandardScaler()
# Fitting and transforming the 'Amount' and 'Time' features
transactions[['Amount', 'Time']] = scaler.fit_transform(transactions[['Amount', 'Time']])

"""StandardScaler standardizes features by removing the mean and scaling to unit variance.
This is often preferred for machine learning algorithms that assume features are normally distributed,
such as logistic regression or support vector machines.

In the context of credit card fraud detection, the 'Amount' feature can have a wide range of values,
and 'Time' is measured in seconds and can also have a large range.
Scaling these features helps to prevent features with larger ranges from dominating the learning process
and improves the performance of many machine learning models.
"""

# Applying PCA for dimensionality reduction
pca = PCA(n_components=2)
transactions_pca = pca.fit_transform(transactions.drop('Class', axis=1))
# Visualizing the data in two dimensions
plt.figure(figsize=(8, 6))
plt.scatter(transactions_pca[transactions['Class'] == 0, 0], transactions_pca[transactions['Class'] == 0, 1], label='Non-Fraudulent', alpha=0.5)
plt.scatter(transactions_pca[transactions['Class'] == 1, 0], transactions_pca[transactions['Class'] == 1, 1], label='Fraudulent', alpha=0.5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Visualization of Credit Card Transactions')
plt.legend()
plt.show()

"""**Anomaly Detection Model**"""

from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
# Isolation Forest
iso_forest = IsolationForest(contamination=0.01)
iso_forest.fit(transactions.drop('Class', axis=1))
iso_predictions = iso_forest.predict(transactions.drop('Class', axis=1))
# Local Outlier Factor
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)
lof_predictions = lof.fit_predict(transactions.drop('Class', axis=1))
# Converting predictions to binary (1 for anomaly, 0 for normal)
iso_predictions[iso_predictions == 1] = 0
iso_predictions[iso_predictions == -1] = 1
lof_predictions[lof_predictions == 1] = 0
lof_predictions[lof_predictions == -1] = 1

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
# Calculating evaluation metrics for Isolation Forest
iso_precision = precision_score(transactions['Class'], iso_predictions)
iso_recall = recall_score(transactions['Class'], iso_predictions)
iso_f1 = f1_score(transactions['Class'], iso_predictions)
iso_roc_auc = roc_auc_score(transactions['Class'], iso_predictions)
# Calculating evaluation metrics for Local Outlier Factor
lof_precision = precision_score(transactions['Class'], lof_predictions)
lof_recall = recall_score(transactions['Class'], lof_predictions)
lof_f1 = f1_score(transactions['Class'], lof_predictions)
lof_roc_auc = roc_auc_score(transactions['Class'], lof_predictions)
# Printing the results
print("Isolation Forest:")
print("Precision:", iso_precision)
print("Recall:", iso_recall)
print("F1-Score:", iso_f1)
print("ROC-AUC:", iso_roc_auc)
print("\n")
print("Local Outlier Factor:")
print("Precision:", lof_precision)
print("Recall:", lof_recall)
print("F1-Score:", lof_f1)
print("ROC-AUC:", lof_roc_auc)

"""**Visualizing Anomalies**"""

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.scatter(transactions_pca[iso_predictions == 0, 0], transactions_pca[iso_predictions == 0, 1], label='Normal', alpha=0.5)
plt.scatter(transactions_pca[iso_predictions == 1, 0], transactions_pca[iso_predictions == 1, 1], label='Fraudulent', alpha=0.5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Isolation Forest Predictions')
plt.legend()
plt.show()

"""**Write a function that accepts a new dataset of credit card transactions and the trained anomaly detection model, returning a list of transactions classified as fraudulent.**"""

#function to identify fraudulent transactions
def detect_fraud(new_transactions, model):
  # Preprocessing the new transactions
  new_transactions[['Amount', 'Time']] = scaler.transform(new_transactions[['Amount', 'Time']])
  # Predicting anomalies
  predictions = model.predict(new_transactions.drop('Class', axis=1, errors='ignore'))
  # Converting predictions to binary (1 for anomaly, 0 for normal)
  predictions[predictions == 1] = 0
  predictions[predictions == -1] = 1
  # Returning indices of fraudulent transactions
  fraudulent_indices = new_transactions[predictions == 1].index.tolist()
  return fraudulent_indices